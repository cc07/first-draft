---
layout: post
title:  "OCR with Deep learning"
date:   2018-05-18 12:07:00 +0800
tags: [ ocr, tensorflow, cnn, lstm, ctc, deeplearning ]
---

# OCR with Deep learning

This article will cover the implementation of text recognition (OCR) by using CNN and LSTM with Tensorflow. In particular, we will attempt to employ Connectionist Temporal Classification (CTC), a state of art text recognition method, to train our model.

*For those who don't yet familiar with Deep learning, Tensorflow, Convolutional Neural Network(CNN) or Long Short Term Memory (LSTM), please them check out first.*

The choice of using CTC for OCR is simply that it is segmentation-free that able to train without the needs to break each character down, and produce a sequence of outputs.

#### Dataset

The sample images were generated with varied length of numbers and letters. All images have fixed height, but varied width given its content.

As the needs of CTC, it is required to provide a sequential input for prediction. In our case, it is the width of the images. Therefore, all images have to transpose from (height, width, channel) to (width, height, channel).

```
images = np.transpose(images, [0, 2, 1])
```

#### Label encoding

The truth labels are encoded to its ACSII code for each character, then subtracted by the first index. Hence the count can be started from index 0.

```
FIRST_INDEX = ord(' ')
n_classes = ord('~') - FIRST_INDEX
```

#### Model structure

The model consists 3 residual blocks, having 5, 10 and 5 layers respectively. And a max-pooling layer following each residual block. All convolution layers have the same architecture that using 3x3 kernerl filters.

The extracted features from CNN will then pass to multi -layers bi-directional LSTM, containing 512 units for both forward and backward cell. After that, the concated rnn outputs will send to CTC for loss calculation and proceed the decoding process.

#### Data pre-process

First, for performance efficient, all pixels in the sample images were normalized to [-0.5, 0.5].

```
images = (np.array(images) / 255) - 0.5
```

Then the images split into training set, testing set and validation set in 5:3:2 ratio.

During the training process, images were further divided into batches for 32 samples each.

As mentioned, each image have fixed height and varied width, we have to standardize the width before putting it into training. We computed the max width for each batch, then padded with zero for which images are shorter than that.

#### Training

The whole training process was carried out on a AWS p2.xlarge instance. The loss of CTC used to decline for the early stage but then stagnant over a long period. It will not like any other deep learning task that improving gradually, however, it will get there at the end.

#### Lession learned

1. As the CTC decoder is only possible to produce one output for each timeframe, hence the sequential length of inputs passing into LSTM cannot be less than the length of the expected values.

2. Even though the mighty powerful of a deep learning network, the choice and quality of dataset remain the crucial criteria for which to build a successful working model.

3. Prove the model is working before adding more layers, noise, or dropout to the training data.

#### More to add

To trackle the potential image orientation problem, we might need to have another model to detect the orientation and correct it before fitting into the CTC model.


#### Links:

1. A Beginner's Guide To Understanding Convolutional Neural Networks, <https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/>
2. CTC, <https://www.silversparro.com/single-post/2018/01/26/Making-CTCs-Deep-Learning-Function-work-for-Speech-Recognition>
3. Improving End-to-End Speech Recognition with Policy Learning, <https://arxiv.org/pdf/1712.07101.pdf>
